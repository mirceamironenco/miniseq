## RL Examples

[GRPO](https://arxiv.org/abs/2402.03300) is a variant of [PPO](https://arxiv.org/abs/1707.06347) which replaces the critic network with a Monte Carlo estimate of the value function. 

<summary><h3>Countdown GRPO</h3></summary>

To run the default GRPO recipe on Countdown:

```bash
bash run_countdown_qwen2.5-3b.sh
```

For distirbuted training pass the launcher command after the filename additionally ending with the end-of-options delimiter `--`. For example, to run on 2 GPUs:

```bash
bash run_countdown_qwen2.5-3b.sh torchrun --nproc-per-node=2 --
```

We compare the effect of different data packing strategies and attention implementation choices:

<img width="949" height="663" alt="Screenshot 2025-09-28 at 17 43 18" src="https://github.com/user-attachments/assets/95620973-70a6-48ab-9d22-0c146ec69313" />

The above plots  can be reproduced with the following comands (full wandb logs [here](https://wandb.ai/mirceam/miniseq_rl)):

<table>
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<details>
<summary><code>bash run_countdown_qwen2.5-3b.sh ...</code></summary>
<pre><code>bash run_countdown_qwen2.5-3b.sh --prefix_share=True --model.flex_attention=True</code></pre>
</details>
</td>
<td>Uses <b>prefix sharing</b> and <b>Flex Attention</b>.</td>
</tr>
<tr>
<td>
<details>
<summary><code>bash run_countdown_qwen2.5-3b.sh ...</code></summary>
<pre><code>bash run_countdown_qwen2.5-3b.sh --prefix_share=False --model.flash_attention2=True --wandb.run_name=qwen3b-fa2</code></pre>
</details>
</td>
<td>Uses <b>Flash Attention v2</b> and packed batches.</td>
</tr>
<tr>
<td>
<details>
<summary><code>bash run_countdown_qwen2.5-3b.sh ...</code></summary>
<pre><code>bash run_countdown_qwen2.5-3b.sh --prefix_share=False --model.flex_attention=True --wandb.run_name=qwen3b-flex</code></pre>
</details>
</td>
<td>Uses <b>Flex Attention</b> and packed batches.</td>
</tr>
</tbody>
</table>

To use wandb, make sure the `WANDB_API_KEY` env. variable is set, otherwise remove the `--wandb.run_name=...` overrides.

<details>



<summary><h3>Reproducing <a href="https://arxiv.org/abs/2503.20783">dr. GRPO</a></h3></summary>

In [dr. GRPO](https://arxiv.org/abs/2503.20783), the authors posit that the original GRPO objective has a response-level length bias due to the normalization by each completion length. They further argue that in the calculation of the advantage, normalizing the rewards by their standard deviation causes a question-level difficulty bias, leading to disproportionate policy updates for questions with low std (i.e. very easy or very hard). 
The proposed corrections are integrated into our GRPO implementation.

To reproduce the results of Table 4 on Qwen2.5-Math-1.5B run:

```bash
bash run_drgrpo.sh --grpo.std_normalize_advantage=False --valid_data.datasets aime24 amc minerva math500 olympiad
```

</details>

<details>
<summary><h3>GSM8k</h3></summary>

[GSM8k](https://huggingface.co/datasets/openai/gsm8k) is a dataset of ~8.5k high-quality, math word problems designed for the grade school level. It was created to train and evaluate the multi-step mathematical reasoning capabilities of large language models.
We follow [verl](https://github.com/volcengine/verl) implementing the environment as a standard showcase for training LLMs via RL. The table below showcases commands used to apply GRPO to models with sizes ranging from 0.5B to 14B.

<table>
<thead>
<tr>
<th>Model</th>
<th>Method</th>
<th>Command</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qwen2.5-0.5B-Instruct</td>
<td>GRPO</td>
<td><code>bash run_gsm8k.sh</code></td>
</tr>
<tr>
<td>Qwen2.5-1.5B-Instruct</td>
<td>GRPO</td>
<td>
<details>
<summary><code>bash run_gsm8k.sh ...</code></summary>
<pre><code>bash run_gsm8k.sh --model.name=qwen2.5-1.5b-instruct</code></pre>
</details>
</td>
</tr>
<tr>
<td>Qwen2.5-3B-Instruct</td>
<td>GRPO</td>
<td>
<details>
<summary><code>bash run_gsm8k.sh ...</code></summary>
<pre><code>bash run_gsm8k.sh --model.name=qwen2.5-3b-instruct</code></pre>
</details>
</td>
</tr>
<tr>
<td>Qwen2.5-7B-Instruct</td>
<td>GRPO-LoRA</td>
<td>
<details>
<summary><code>bash run_gsm8k.sh ...</code></summary>
<pre><code>bash run_gsm8k.sh torchrun --nproc-per-node=2 -- lora:on --model.name=qwen2.5-7b-instruct --train.device_batch_size=64</code></pre>
</details>
</td>
</tr>
<tr>
<td>Qwen2.5-14B-Instruct</td>
<td>GRPO-LoRA</td>
<td>
<details>
<summary><code>bash run_gsm8k.sh ...</code></summary>
<pre><code>bash run_gsm8k.sh torchrun --nproc-per-node=2 -- lora:on --model.name=qwen2.5-14b-instruct --train.device_batch_size=64</code></pre>
</details>
</td>
</tr>
</tbody>
</table>

</details>

## Preference Optimization Examples

<details>
<summary><h3>DPO on UltraFeedback</a></h3></summary>

In [ultrafeedback.py](./ultrafeedback.py) the standard DPO recipe is showcased. We directly reference and use the schema defined on HF at [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) to process the dataset items:


```py
# Schema on huggingface (optional).
UFItem = TypedDict[{"prompt": str, "chosen": list[Message], "rejected": list[Message]}]


def uf_transform(example: UFItem) -> PreferenceDict:
    # Assume single-turn
    chosen, rejected = filter(
        lambda x: x["role"] == "assistant", example["chosen"] + example["rejected"]
    )

    return {
        "prompt": example["prompt"],
        "chosen": chosen["content"],
        "rejected": rejected["content"],
    }


@dataclasses.dataclass
class Config(recipes.PreferenceRecipeConfig):
    train_data: cfg.data.PreferenceDatasetConfig = cfg.data.PreferenceDatasetConfig(
        name="HuggingFaceH4/ultrafeedback_binarized",
        split="train_prefs",
        preference_map=uf_transform,
        max_seqlen=2049,
    )


config = cli.run_default_cli(Config, console_outputs=on_local_rank_zero())
trainer = recipes.create_preference_trainer(config)
trainer.run()
```

To run on 4 GPUs:

```bash
torchrun --nproc-per-node=4 ultrafeedback.py --model.name=qwen2.5-7b-instruct --packed=True --model.flash_attention2=True
```


</details>